@inproceedings{dwork2012fairness,
    title = {Fairness through awareness},
    author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold
              , Omer and Zemel, Richard},
    booktitle = {Proceedings of the 3rd innovations in theoretical computer
                 science conference},
    pages = {214--226},
    year = {2012},
}

@article{friedler2016possibility,
    title = {On the (im) possibility of fairness},
    author = {Friedler, Sorelle A and Scheidegger, Carlos and Venkatasubramanian
              , Suresh},
    journal = {arXiv preprint arXiv:1609.07236},
    year = {2016},
}

@inproceedings{kleinberg2018inherent,
    title = {Inherent trade-offs in algorithmic fairness},
    author = {Kleinberg, Jon},
    booktitle = {Abstracts of the 2018 ACM International Conference on
                 Measurement and Modeling of Computer Systems},
    pages = {40--40},
    year = {2018},
}

@inproceedings{nips2017preproc,
    author = {Calmon, Flavio and Wei, Dennis and Vinzamuri, Bhanukiran and
              Natesan Ramamurthy, Karthikeyan and Varshney, Kush R},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R.
              Fergus and S. Vishwanathan and R. Garnett},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {Optimized Pre-Processing for Discrimination Prevention},
    url = {
           https://proceedings.neurips.cc/paper/2017/file/9a49a25d845a483fae4be7e341368e36-Paper.pdf
           },
    volume = {30},
    year = {2017},
}

@article{Kamiran2011DataPT,
    title = {Data preprocessing techniques for classification without
             discrimination},
    author = {Faisal Kamiran and Toon Calders},
    journal = {Knowledge and Information Systems},
    year = {2011},
    volume = {33},
    pages = {1-33},
}

@inproceedings{zemel2013learning,
    title = {Learning fair representations},
    author = {Zemel, Rich and Wu, Yu and Swersky, Kevin and Pitassi, Toni and
              Dwork, Cynthia},
    booktitle = {International conference on machine learning},
    pages = {325--333},
    year = {2013},
    organization = {PMLR},
}

@article{donini2018empirical,
    title = {Empirical risk minimization under fairness constraints},
    author = {Donini, Michele and Oneto, Luca and Ben-David, Shai and
              Shawe-Taylor, John S and Pontil, Massimiliano},
    journal = {Advances in Neural Information Processing Systems},
    volume = {31},
    year = {2018},
}

@inproceedings{zafar2017fairness,
    title = {Fairness constraints: Mechanisms for fair classification},
    author = {Zafar, Muhammad Bilal and Valera, Isabel and Rogriguez, Manuel
              Gomez and Gummadi, Krishna P},
    booktitle = {Artificial intelligence and statistics},
    pages = {962--970},
    year = {2017},
    organization = {PMLR},
}

@article{zafar2019fairness,
    author = {Muhammad Bilal Zafar and Isabel Valera and Manuel Gomez-Rodriguez
              and Krishna P. Gummadi},
    title = { Fairness Constraints: A Flexible Approach for Fair Classification
             },
    journal = {Journal of Machine Learning Research},
    year = {2019},
    volume = {20},
    number = {75},
    pages = {1--42},
    url = {http://jmlr.org/papers/v20/18-262.html},
}

@article{MacCarthy2018StandardsOF,
    title = {Standards of Fairness for Disparate Impact Assessment of Big Data
             Algorithms},
    author = {Mark MacCarthy},
    journal = {Other Information Systems \& eBusiness eJournal},
    year = {2018},
}

@article{Dimitrakakis_Liu_Parkes_Radanovic_2019,
    title = {Bayesian Fairness},
    volume = {33},
    url = {https://ojs.aaai.org/index.php/AAAI/article/view/3824},
    DOI = {10.1609/aaai.v33i01.3301509},
    abstractNote = {&lt;p&gt;We consider the problem of how decision making can
                    be fair when the underlying probabilistic model of the world
                    is not known with certainty. We argue that recent notions of
                    fairness in machine learning need to explicitly incorporate
                    parameter uncertainty, hence we introduce the notion of
                    &lt;em&gt;Bayesian fairness&lt;/em&gt; as a suitable
                    candidate for fair decision rules. Using
                    &lt;em&gt;balance&lt;/em&gt;, a definition of fairness
                    introduced in (Kleinberg, Mullainathan, and Raghavan 2016),
                    we show how a Bayesian perspective can lead to
                    well-performing and fair decision rules even under high
                    uncertainty.&lt;/p&gt;},
    number = {01},
    journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
    author = {Dimitrakakis, Christos and Liu, Yang and Parkes, David C. and
              Radanovic, Goran},
    year = {2019},
    month = {07},
    pages = {509-516},
}

@article{polikar2006ensemble,
    title = {Ensemble based systems in decision making},
    author = {Polikar, Robi},
    journal = {IEEE Circuits and systems magazine},
    volume = {6},
    number = {3},
    pages = {21--45},
    year = {2006},
    publisher = {IEEE},
}

@article{schapire1990strength,
    title = {The strength of weak learnability},
    author = {Schapire, Robert E},
    journal = {Machine learning},
    volume = {5},
    number = {2},
    pages = {197--227},
    year = {1990},
    publisher = {Springer},
}

@article{breiman1996bagging,
    title = {Bagging predictors},
    author = {Breiman, Leo},
    journal = {Machine learning},
    volume = {24},
    number = {2},
    pages = {123--140},
    year = {1996},
    publisher = {Springer},
}

@inproceedings{dietterich2000ensemble,
    author = "Dietterich, Thomas G.",
    title = "Ensemble Methods in Machine Learning",
    booktitle = "Multiple Classifier Systems",
    year = "2000",
    publisher = "Springer Berlin Heidelberg",
    address = "Berlin, Heidelberg",
    pages = "1--15",
    abstract = "Ensemble methods are learning algorithms that construct a set of
                classifiers and then classify new data points by taking a
                (weighted) vote of their predictions. The original ensemble
                method is Bayesian averaging, but more recent algorithms include
                error-correcting output coding, Bagging, and boosting. This paper
                reviews these methods and explains why ensembles can often
                perform better than any single classifier. Some previous studies
                comparing ensemble methods are reviewed, and some new experiments
                are presented to uncover the reasons that Adaboost does not
                overfit rapidly.",
    isbn = "978-3-540-45014-6",
}

@article{livieris2020ensemble,
    title = {Ensemble deep learning models for forecasting cryptocurrency
             time-series},
    author = {Livieris, Ioannis E and Pintelas, Emmanuel and Stavroyiannis,
              Stavros and Pintelas, Panagiotis},
    journal = {Algorithms},
    volume = {13},
    number = {5},
    pages = {121},
    year = {2020},
    publisher = {Multidisciplinary Digital Publishing Institute},
}

 @article{JIN2022105560,
    title = {A data-driven hybrid ensemble AI model for COVID-19 infection
             forecast using multiple neural networks and reinforced learning},
    journal = {Computers in Biology and Medicine},
    volume = {146},
    pages = {105560},
    year = {2022},
    issn = {0010-4825},
    doi = {https://doi.org/10.1016/j.compbiomed.2022.105560},
    url = {https://www.sciencedirect.com/science/article/pii/S0010482522003523},
    author = {Weiqiu Jin and Shuqing Dong and Chengqing Yu and Qingquan Luo},
    keywords = {COVID-19, Infection prediction, Hybrid model, Artificial
                intelligence},
    abstract = {The COVID-19 outbreak poses a huge challenge to international
                public health. Reliable forecast of the number of cases is of
                great significance to the planning of health resources and the
                investigation and evaluation of the epidemic situation. The
                data-driven machine learning models can adapt to complex changes
                in the epidemic situation without relying on correct physical
                dynamics modeling, which are sensitive and accurate in predicting
                the development of the epidemic. In this paper, an ensemble
                hybrid model based on Temporal Convolutional Networks (TCN),
                Gated Recurrent Unit (GRU), Deep Belief Networks (DBN),
                Q-learning, and Support Vector Machine (SVM) models, namely
                TCN-GRU-DBN-Q-SVM model, is proposed to achieve the forecasting
                of COVID-19 infections. Three widely-used predictors, TCN, GRU,
                and DBN are used as elements of the hybrid model ensembled by the
                weights provided by reinforcement learning method. Furthermore,
                an error predictor built by SVM, is trained with validation set,
                and the final prediction result could be obtained by combining
                the TCN-GRU-DBN-Q model with the SVM error predictor. In order to
                investigate the forecasting performance of the proposed hybrid
                model, several comparison models (TCN-GRU-DBN-Q, LSTM, N-BEATS,
                ANFIS, VMD-BP, WT-RVFL, and ARIMA models) are selected. The
                experimental results show that: (1) the prediction effect of the
                TCN-GRU-DBN-Q-SVM model on COVID-19 infection is satisfactory,
                which has been verified in three national infection data from the
                UK, India, and the US, and the proposed model has good
                generalization ability; (2) in the proposed hybrid model, SVM can
                efficiently predict the possible error of the predicted series
                given by TCN-GRU-DBN-Q components; (3) the integrated weights
                based on Q-learning can be adaptively adjusted according to the
                characteristics of the data in the forecasting tasks in different
                countries and multiple situations, which ensures the accuracy,
                robustness and generalization of the proposed model.},
}

@article{CHOWDHURY2022105405,
    title = {Machine learning for detecting COVID-19 from cough sounds: An
             ensemble-based MCDM method},
    journal = {Computers in Biology and Medicine},
    volume = {145},
    pages = {105405},
    year = {2022},
    issn = {0010-4825},
    doi = {https://doi.org/10.1016/j.compbiomed.2022.105405},
    url = {https://www.sciencedirect.com/science/article/pii/S0010482522001974},
    author = {Nihad Karim Chowdhury and Muhammad Ashad Kabir and Md. Muhtadir
              Rahman and Sheikh Mohammed Shariful Islam},
    keywords = {Classification, Cough, COVID-19, Ensemble, Entropy, Machine
                learning, MCDM, TOPSIS},
    abstract = {This research aims to analyze the performance of
                state-of-the-art machine learning techniques for classifying
                COVID-19 from cough sounds and to identify the model(s) that
                consistently perform well across different cough datasets.
                Different performance evaluation metrics (precision, sensitivity,
                specificity, AUC, accuracy, etc.) make selecting the best
                performance model difficult. To address this issue, in this paper
                , we propose an ensemble-based multi-criteria decision making
                (MCDM) method for selecting top performance machine learning
                technique(s) for COVID-19 cough classification. We use four cough
                datasets, namely Cambridge, Coswara, Virufy, and NoCoCoDa to
                verify the proposed method. At first, our proposed method uses
                the audio features of cough samples and then applies machine
                learning (ML) techniques to classify them as COVID-19 or
                non-COVID-19. Then, we consider a multi-criteria decision-making
                (MCDM) method that combines ensemble technologies (i.e., soft and
                hard) to select the best model. In MCDM, we use the technique for
                order preference by similarity to ideal solution (TOPSIS) for
                ranking purposes, while entropy is applied to calculate
                evaluation criteria weights. In addition, we apply the feature
                reduction process through recursive feature elimination with
                cross-validation under different estimators. The results of our
                empirical evaluations show that the proposed method outperforms
                the state-of-the-art models. We see that when the proposed method
                is used for analysis using the Extra-Trees classifier, it has
                achieved promising results (AUC: 0.95, Precision: 1, Recall:
                0.97).},
}

@article{huang17snapshot,
    author = {Gao Huang and Yixuan Li and Geoff Pleiss and Zhuang Liu and John
              E. Hopcroft and Kilian Q. Weinberger},
    title = {Snapshot Ensembles: Train 1, get {M} for free},
    journal = {CoRR},
    volume = {abs/1704.00109},
    year = {2017},
    url = {http://arxiv.org/abs/1704.00109},
    eprinttype = {arXiv},
    eprint = {1704.00109},
    timestamp = {Mon, 10 Sep 2018 15:49:32 +0200},
    biburl = {https://dblp.org/rec/journals/corr/HuangLPLHW17.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}

@inproceedings{autoweka,
    author = {Thornton, Chris and Hutter, Frank and Hoos, Holger H. and
              Leyton-Brown, Kevin},
    title = {Auto-WEKA: Combined Selection and Hyperparameter Optimization of
             Classification Algorithms},
    year = {2013},
    isbn = {9781450321747},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2487575.2487629},
    doi = {10.1145/2487575.2487629},
    abstract = {Many different machine learning algorithms exist; taking into
                account each algorithm's hyperparameters, there is a staggeringly
                large number of possible alternatives overall. We consider the
                problem of simultaneously selecting a learning algorithm and
                setting its hyperparameters, going beyond previous work that
                attacks these issues separately. We show that this problem can be
                addressed by a fully automated approach, leveraging recent
                innovations in Bayesian optimization. Specifically, we consider a
                wide range of feature selection techniques (combining 3 search
                and 8 evaluator methods) and all classification approaches
                implemented in WEKA's standard distribution, spanning 2 ensemble
                methods, 10 meta-methods, 27 base classifiers, and hyperparameter
                settings for each classifier. On each of 21 popular datasets from
                the UCI repository, the KDD Cup 09, variants of the MNIST dataset
                and CIFAR-10, we show classification performance often much
                better than using standard selection and hyperparameter
                optimization methods. We hope that our approach will help
                non-expert users to more effectively identify machine learning
                algorithms and hyperparameter settings appropriate to their
                applications, and hence to achieve improved performance.},
    booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on
                 Knowledge Discovery and Data Mining},
    pages = {847–855},
    numpages = {9},
    keywords = {hyperparameter optimization, weka, model selection},
    location = {Chicago, Illinois, USA},
    series = {KDD '13},
}

@article{yang2010bioinformatics,
    Title = {A Review of Ensemble Methods in Bioinformatics},
    Journal = {Current Bioinformatics},
    volume = {5},
    Number = {4},
    Pages = {296-308},
    Year = {2010},
    ISSN = {1574-8936/2212-392X},
    DOI = {10.2174/157489310794072508},
    URL = {http://www.eurekaselect.com/article/32231},
    Author = {Yang, Pengyi and Hwa Yang, Yee and B. Zhou, Bing and Y. Zomaya,
              Albert},
    Keywords = {Ensemble learning, bioinformatics, microarray, mass
                spectrometry-based proteomics, gene-gene interaction, regulatory,
                elements prediction, ensemble of support vector machines, meta
                ensemble, ensemble feature selection.},
    Abstract = {Ensemble learning is an intensively studied technique in machine
                learning and pattern recognition. Recent work in computational
                biology has seen an increasing use of ensemble learning methods
                due to their unique advantages in dealing with small sample size,
                high-dimensionality, and complex data structures. The aim of this
                article is two-fold. Firstly, it is to provide a review of the
                most widely used ensemble learning methods and their application
                in various bioinformatics problems, including the main topics of
                gene expression, mass spectrometry-based proteomics, gene-gene
                interaction identification from genome-wide association studies,
                and prediction of regulatory elements from DNA and protein
                sequences. Secondly, we try to identify and summarize future
                trends of ensemble methods in bioinformatics. Promising
                directions such as ensemble of support vector machines,
                meta-ensembles, and ensemble based feature selection are
                discussed.},
}

@article{weka,
    author = {Witten, Ian and Hall, Mark and Frank, Eibe and Holmes, Geoffrey
              and Pfahringer, Bernhard and Reutemann, Peter},
    year = {2009},
    month = {11},
    pages = {10-18},
    title = {The WEKA data mining software: An update},
    volume = {11},
    journal = {SIGKDD Explorations},
    doi = {10.1145/1656274.1656278},
}

@article{feurer2015efficient,
    title = {Efficient and robust automated machine learning},
    author = {Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and
              Springenberg, Jost and Blum, Manuel and Hutter, Frank},
    journal = {Advances in neural information processing systems},
    volume = {28},
    year = {2015},
}

@article{autoKeras,
    author = {Haifeng Jin and Qingquan Song and Xia Hu},
    title = {Efficient Neural Architecture Search with Network Morphism},
    journal = {CoRR},
    volume = {abs/1806.10282},
    year = {2018},
    url = {http://arxiv.org/abs/1806.10282},
    eprinttype = {arXiv},
    eprint = {1806.10282},
    timestamp = {Mon, 13 Aug 2018 16:46:49 +0200},
    biburl = {https://dblp.org/rec/journals/corr/abs-1806-10282.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}

@article{pedregosa2011scikit,
    title = {Scikit-learn: Machine learning in Python},
    author = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre
              and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and
              Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg
              , Vincent and others},
    journal = {the Journal of machine Learning research},
    volume = {12},
    pages = {2825--2830},
    year = {2011},
    publisher = {JMLR. org},
}

@online{chollet2015keras,
    title = {Keras},
    url = {https://github.com/fchollet/keras},
}

@article{NAS,
    doi = {10.48550/ARXIV.1808.05377},
    url = {https://arxiv.org/abs/1808.05377},
    author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
    keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), Neural and
                Evolutionary Computing (cs.NE), FOS: Computer and information
                sciences, FOS: Computer and information sciences},
    title = {Neural Architecture Search: A Survey},
    publisher = {arXiv},
    year = {2018},
    copyright = {arXiv.org perpetual, non-exclusive license},
}

@inproceedings{autogoal,
    author = {Est{\'e}vez Velarde, Suilan and Guti{\'e}rrez, Yoan and Montoyo,
              Andr{\'e}s and Almeida Cruz, Yudivi{\'a}n},
    year = {2020},
    month = {01},
    pages = {3558-3568},
    title = {Automatic Discovery of Heterogeneous Machine Learning Pipelines: An
             Application to Natural Language Processing},
    doi = {10.18653/v1/2020.coling-main.317},
}

@article{estevez2020general,
    title = {General-purpose hierarchical optimisation of machine learning
             pipelines with grammatical evolution},
    author = {Est{\'e}vez-Velarde, Suilan and Guti{\'e}rrez, Yoan and
              Almeida-Cruz, Yudivi{\'a}n and Montoyo, Andr{\'e}s},
    journal = {Information Sciences},
    year = {2020},
    publisher = {Elsevier},
    doi = {10.1016/j.ins.2020.07.035},
}

@inproceedings{estevez2020automatic,
  title={Automatic Discovery of Heterogeneous Machine Learning Pipelines: An Application to Natural Language Processing},
  author={Estevez-Velarde, Suilan and Guti{\'e}rrez, Yoan and Montoyo, Andr{\'e}s and Almeida-Cruz, Yudivi{\'a}n},
  booktitle={Proceedings of the 28th International Conference on Computational Linguistics},
  pages={3558--3568},
  year={2020}
}

@article{nltk,
    author = {Edward Loper and Steven Bird},
    title = {{NLTK:} The Natural Language Toolkit},
    journal = {CoRR},
    volume = {cs.CL/0205028},
    year = {2002},
    url = {https://arxiv.org/abs/cs/0205028},
    timestamp = {Fri, 10 Jan 2020 12:59:08 +0100},
    biburl = {https://dblp.org/rec/journals/corr/cs-CL-0205028.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}


@article{paszke2019pytorch,
    title = {Pytorch: An imperative style, high-performance deep learning
             library},
    author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam
              and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin
              , Zeming and Gimelshein, Natalia and Antiga, Luca and others},
    journal = {Advances in neural information processing systems},
    volume = {32},
    year = {2019},
}

@article{deb2002nsgaii,
    author = {Deb, K. and Pratap, A. and Agarwal, S. and Meyarivan, T.},
    journal = {IEEE Transactions on Evolutionary Computation},
    title = {A fast and elitist multiobjective genetic algorithm: NSGA-II},
    year = {2002},
    volume = {6},
    number = {2},
    pages = {182-197},
    abstract = {Multi-objective evolutionary algorithms (MOEAs) that use
                non-dominated sorting and sharing have been criticized mainly
                for: (1) their O(MN/sup 3/) computational complexity (where M is
                the number of objectives and N is the population size); (2) their
                non-elitism approach; and (3) the need to specify a sharing
                parameter. In this paper, we suggest a non-dominated
                sorting-based MOEA, called NSGA-II (Non-dominated Sorting Genetic
                Algorithm II), which alleviates all of the above three
                difficulties. Specifically, a fast non-dominated sorting approach
                with O(MN/sup 2/) computational complexity is presented. Also, a
                selection operator is presented that creates a mating pool by
                combining the parent and offspring populations and selecting the
                best N solutions (with respect to fitness and spread). Simulation
                results on difficult test problems show that NSGA-II is able, for
                most problems, to find a much better spread of solutions and
                better convergence near the true Pareto-optimal front compared to
                the Pareto-archived evolution strategy and the strength-Pareto
                evolutionary algorithm - two other elitist MOEAs that pay special
                attention to creating a diverse Pareto-optimal front. Moreover,
                we modify the definition of dominance in order to solve
                constrained multi-objective problems efficiently. Simulation
                results of the constrained NSGA-II on a number of test problems,
                including a five-objective, seven-constraint nonlinear problem,
                are compared with another constrained multi-objective optimizer,
                and the much better performance of NSGA-II is observed.},
    keywords = {},
    doi = {10.1109/4235.996017},
    ISSN = {1941-0026},
    month = {04},
}

@inproceedings{agarwal2018reductions,
    title = {A reductions approach to fair classification},
    author = {Agarwal, Alekh and Beygelzimer, Alina and Dud{\'i}k, Miroslav and
              Langford, John and Wallach, Hanna},
    booktitle = {International Conference on Machine Learning},
    pages = {60--69},
    year = {2018},
    organization = {PMLR},
}

@inproceedings{agarwal2019fair,
    title = {Fair regression: Quantitative definitions and reduction-based
             algorithms},
    author = {Agarwal, Alekh and Dud{\'i}k, Miroslav and Wu, Zhiwei Steven},
    booktitle = {International Conference on Machine Learning},
    pages = {120--129},
    year = {2019},
    organization = {PMLR},
}

@inproceedings{kearns2018preventing,
    title = {Preventing fairness gerrymandering: Auditing and learning for
             subgroup fairness},
    author = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei
              Steven},
    booktitle = {International Conference on Machine Learning},
    pages = {2564--2572},
    year = {2018},
    organization = {PMLR},
}

@article{thomas2019preventing,
    title = {Preventing undesirable behavior of intelligent machines},
    author = {Thomas, Philip S and Castro da Silva, Bruno and Barto, Andrew G
              and Giguere, Stephen and Brun, Yuriy and Brunskill, Emma},
    journal = {Science},
    volume = {366},
    number = {6468},
    pages = {999--1004},
    year = {2019},
    publisher = {American Association for the Advancement of Science},
}

@inproceedings{chiappa2018causal,
    title = {A causal Bayesian networks viewpoint on fairness},
    author = {Chiappa, Silvia and Isaac, William S},
    booktitle = {IFIP International Summer School on Privacy and Identity
                 Management},
    pages = {3--20},
    year = {2018},
    organization = {Springer},
}

@book{miettinen2012nonlinear,
    title = {Nonlinear multiobjective optimization},
    author = {Miettinen, Kaisa},
    volume = {12},
    year = {2012},
    publisher = {Springer Science \& Business Media},
}

@incollection{kuhn2014nonlinear,
    title = {Nonlinear programming},
    author = {Kuhn, Harold W and Tucker, Albert W},
    booktitle = {Traces and emergence of nonlinear programming},
    pages = {247--258},
    year = {2014},
    publisher = {Springer},
}

@inproceedings{schutze2005continuation,
    author = {Sch\"{u}tze, Oliver and Dell'Aere, Alessandro and Dellnitz,
              Michael},
    title = {{On Continuation Methods for the Numerical Treatment of
             Multi-Objective Optimization Problems}},
    booktitle = {Practical Approaches to Multi-Objective Optimization},
    pages = {1--15},
    series = {Dagstuhl Seminar Proceedings (DagSemProc)},
    ISSN = {1862-4405},
    year = {2005},
    volume = {4461},
    editor = {J\"{u}rgen Branke and Kalyanmoy Deb and Kaisa Miettinen and Ralph
              E. Steuer},
    publisher = {Schloss Dagstuhl -- Leibniz-Zentrum f{\"u}r Informatik},
    address = {Dagstuhl, Germany},
    URL = {https://drops.dagstuhl.de/opus/volltexte/2005/349},
    URN = {urn:nbn:de:0030-drops-3497},
    doi = {10.4230/DagSemProc.04461.16},
    annote = {Keywords: multi-objective optimization, continuation, k-manifolds},
}

@book{hillermeier2001nonlinear,
    title = {Nonlinear multiobjective optimization: a generalized homotopy
             approach},
    author = {Hillermeier, Claus and others},
    volume = {135},
    year = {2001},
    publisher = {Springer Science \& Business Media},
}

@book{back1996evolutionary,
    title = {Evolutionary algorithms in theory and practice: evolution
             strategies, evolutionary programming, genetic algorithms},
    author = {Back, Thomas},
    year = {1996},
    publisher = {Oxford university press},
}

@article{neill2001pge,
    author = {O'Neill, M. and Ryan, C.},
    journal = {IEEE Transactions on Evolutionary Computation},
    title = {Grammatical evolution},
    year = {2001},
    volume = {5},
    number = {4},
    pages = {349-358},
    doi = {10.1109/4235.942529},
}

@inproceedings{megane2021probabilistic,
    title = {Probabilistic grammatical evolution},
    author = {M{\'e}gane, Jessica and Louren{\c{c}}o, Nuno and Machado, Penousal
              },
    booktitle = {European Conference on Genetic Programming (Part of EvoStar)},
    pages = {198--213},
    year = {2021},
    organization = {Springer},
}

@inproceedings{chiruzzo2019overview,
  title={Overview of HAHA at IberLEF 2019: Humor analysis based on human annotation.},
  author={Chiruzzo, Luis and Castro, Santiago and Etcheverry, Mathias and Garat, Diego and Prada, Juan Jos{\'e} and Ros{\'a}, Aiala},
  booktitle={IberLEF@ SEPLN},
  pages={132--144},
  year={2019}
}

@misc{ucidata,
author = "Dua, Dheeru and Graff, Casey",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{bolukbasi2016man,
  title={Man is to computer programmer as woman is to homemaker? debiasing word embeddings},
  author={Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y and Saligrama, Venkatesh and Kalai, Adam T},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@inproceedings{verma2018fairness,
  title={Fairness definitions explained},
  author={Verma, Sahil and Rubin, Julia},
  booktitle={2018 ieee/acm international workshop on software fairness (fairware)},
  pages={1--7},
  year={2018},
  organization={IEEE}
}

@inproceedings{paria2020flexible,
  title={A flexible framework for multi-objective bayesian optimization using random scalarizations},
  author={Paria, Biswajit and Kandasamy, Kirthevasan and P{\'o}czos, Barnab{\'a}s},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={766--776},
  year={2020},
  organization={PMLR}
}

@article{knowles2006parego,  author={Knowles, J.},  journal={IEEE Transactions on Evolutionary Computation},   title={ParEGO: a hybrid algorithm with on-line landscape approximation for expensive multiobjective optimization problems},   year={2006},  volume={10},  number={1},  pages={50-66},  doi={10.1109/TEVC.2005.851274}}

@inproceedings{FERM,
 author = {Donini, Michele and Oneto, Luca and Ben-David, Shai and Shawe-Taylor, John S and Pontil, Massimiliano},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Empirical Risk Minimization Under Fairness Constraints},
 url = {https://proceedings.neurips.cc/paper/2018/file/83cdcec08fbf90370fcf53bdd56604ff-Paper.pdf},
 volume = {31},
 year = {2018}
}

@inproceedings{zafar2017fairness,
  title={Fairness constraints: Mechanisms for fair classification},
  author={Zafar, Muhammad Bilal and Valera, Isabel and Rogriguez, Manuel Gomez and Gummadi, Krishna P},
  booktitle={Artificial intelligence and statistics},
  pages={962--970},
  year={2017},
  organization={PMLR}
}

@inproceedings{zhang2018mitigating,
  title={Mitigating unwanted biases with adversarial learning},
  author={Zhang, Brian Hu and Lemoine, Blake and Mitchell, Margaret},
  booktitle={Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
  pages={335--340},
  year={2018}
}

@article{chawla2002smote,
  title={SMOTE: synthetic minority over-sampling technique},
  author={Chawla, Nitesh V and Bowyer, Kevin W and Hall, Lawrence O and Kegelmeyer, W Philip},
  journal={Journal of artificial intelligence research},
  volume={16},
  pages={321--357},
  year={2002}
}

@inproceedings{perrone2021fbo,
author = {Perrone, Valerio and Donini, Michele and Zafar, Muhammad Bilal and Schmucker, Robin and Kenthapadi, Krishnaram and Archambeau, C\'{e}dric},
title = {Fair Bayesian Optimization},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462629},
doi = {10.1145/3461702.3462629},
abstract = {Given the increasing importance of machine learning (ML) in our lives, several algorithmic fairness techniques have been proposed to mitigate biases in the outcomes of the ML models. However, most of these techniques are specialized to cater to a single family of ML models and a specific definition of fairness, limiting their adaptibility in practice. We introduce a general constrained Bayesian optimization (BO) framework to optimize the performance of any ML model while enforcing one or multiple fairness constraints. BO is a model-agnostic optimization method that has been successfully applied to automatically tune the hyperparameters of ML models. We apply BO with fairness constraints to a range of popular models, including random forests, gradient boosting, and neural networks, showing that we can obtain accurate and fair solutions by acting solely on the hyperparameters. We also show empirically that our approach is competitive with specialized techniques that enforce model-specific fairness constraints, and outperforms preprocessing methods that learn fair representations of the input data. Moreover, our method can be used in synergy with such specialized fairness techniques to tune their hyperparameters. Finally, we study the relationship between fairness and the hyperparameters selected by BO. We observe a correlation between regularization and unbiased models, explaining why acting on the hyperparameters leads to ML models that generalize well and are fair.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {854–863},
numpages = {10},
keywords = {bias, Bayesian optimization, fairness, autoML, hyperparameter tuning},
location = {Virtual Event, USA},
series = {AIES '21}
}

@inproceedings{goodfellow2014adversarial,
 author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generative Adversarial Nets},
 url = {https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf},
 volume = {27},
 year = {2014}
}

@article{jones1998ego,
author={Jones, Donald R.
and Schonlau, Matthias
and Welch, William J.},
title={Efficient Global Optimization of Expensive Black-Box Functions},
journal={Journal of Global Optimization},
year={1998},
month={Dec},
day={01},
volume={13},
number={4},
pages={455-492},
abstract={In many engineering optimization problems, the number of function evaluations is severely limited by time or cost. These problems pose a special challenge to the field of global optimization, since existing methods often require more function evaluations than can be comfortably afforded. One way to address this challenge is to fit response surfaces to data collected by evaluating the objective and constraint functions at a few points. These surfaces can then be used for visualization, tradeoff analysis, and optimization. In this paper, we introduce the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering. We then show how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule. The key to using response surfaces for global optimization lies in balancing the need to exploit the approximating surface (by sampling where it is minimized) with the need to improve the approximation (by sampling where prediction error may be high). Striking this balance requires solving certain auxiliary problems which have previously been considered intractable, but we show how these computational obstacles can be overcome.},
issn={1573-2916},
doi={10.1023/A:1008306431147},
url={https://doi.org/10.1023/A:1008306431147}
}