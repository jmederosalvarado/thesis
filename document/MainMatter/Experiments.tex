\chapter{Análisis Experimental}\label{chapter:experiments}

En este capitulo se evalúa la capacidad de nuestro sistema de optimización de dos fases de resolver un problema de clasificación y lograr buenos resultados en métricas de precisión y equidad a partir de ensamblar modelos base.
La experimentación realizada consiste de dos etapas, en una se analiza la capacidad de la primera fase del sistema de obtener un conjunto de modelos base lo suficientemente diverso como para que ensamblar estos resultados en un modelo de mayor precisión que los modelos base.
En la segunda etapa, se estudia si el algoritmo propuesto permite encontrar formas de ensamblar los modelos base resultantes de la primera etapa de forma tal que se obtengan valores satisfactorios tanto de precisión como en las métricas de equidad.

\section{Marco Experimental}

Una primera etapa experimental se lleva a cabo con el objetivo de estudiar la capacidad del sistema de producir un conjunto de modelos base capaz de, al ser ensamblado, generalizar y obtener mejores resultados que dichos modelos base.
Adicionalmente se desea estudiar que influencia tienen las distintas métricas de diversidad utilizadas en esta capacidad del sistema.

Para estimar el mejor rendimiento obtenible a partir de ensamblar el conjunto de modelos base encontrado, dos medidas son propuestas a continuación.
Estas medidas estiman el rendimiento que logran modelos de ensemble de prueba, esto es, modelos de ensemble que conocen los resultados correctos a priori, y por tanto son modelos que no tienen utilidad real fuera de ser utilizados con propósitos de comparación.
Dichos modelos de ensemble son llamados \emph{Oráculo Optimista} y \emph{Oráculo Sobre-ajustado}.

\begin{description}

\item[Oráculo Optimista.]
Este modelo oráculo devuelve la etiqueta correcta si al menos uno delos modelos base predijo dicha etiqueta.
La única forma de que este modelo falle es si ninguno de los modelos base fue capaz de sugerir la etiqueta correcta.

\begin{equation*}
O_{optimista}^{(i)}(M) =
    \begin{cases}
        y_i & \textrm{si $y_i \in \{\,y^{(j)}_i \mid m^{(j)} \in M \,\}$} \\
        y^{(0)}_i & \textrm{en otro caso} \\
    \end{cases}
\end{equation*}

Los resultados alcanzados por este ensemble reflejan el grado de cobertura de los modelos base sobre la colección de evaluación, esto es, cuantos ejemplos son correctamente predichos por al menos uno de los clasificadores base.

\item[Oráculo Sobre-ajustado.]
Este modelo oráculo computa todas las combinaciones de salidas de los modelos base y asigna a cada combinación la etiqueta mas frecuente encontrada en el conjunto correspondiente de etiquetas correctas.
Este modelo falla cuando la misma combinación de modelos base tiene que producir diferentes etiquetas para que todas las posibles entradas sean clasificadas correctamente.

\begin{equation*}
O_{sobreajustado}^{(i)}(M) =
    \textbf{max\_count}\left(\left\{\, y_k \,\middle\vert\, (x_k, y_k) \in D, \, \underset{m^{(j)} \in M}{\forall} y^{(j)}_k = y^{(j)}_i \,\right\}\right)
\end{equation*}

La función \textbf{max\_count} devuelve el elemento mas frecuente de la colección (en caso de un empate, siempre devuelve el primero que encuentra).

Los resultados que obtiene este ensemble, proveen una cota superior del mejor rendimiento que puede ser obtenido utilizando un conjunto de reglas \textbf{consistente} para ensamblar la colección de modelos base.

\end{description}

Primeramente se desea validar que nuestro sistema es capaz de generar modelos base en la primera fase que al ser ensamblados en la segunda fase dan lugar a un ensemble que obtiene mejores resultados sobre el conjunto de evaluación que cualquiera de los modelos base de los que se compone.
Para ello, los resultados obtenidos por el ensemble producido por nuestro sistema son comparados con los obtenidos por el mejor de los modelos base, y analizados respecto a los resultados obtenidos por el \emph{Oráculo Sobre-ajustado} descrito anteriormente.

Además en esta etapa experimental se desea también comprobar la influencia de las diferentes métricas de diversidad utilizadas en la capacidad del sistema para producir modelos base que generalicen luego de ser ensamblados.
Con este propósito, se realiza un análisis de que tan bien los modelos base cubren el espacio de entrada de nuestro problema.
Para este análisis resulta de suma utilidad el concepto de \emph{Oráculo Optimista}, el cual nos darán información acerca de la influencia de las distintas métricas de diversidad en la distribución de los distintos modelos base sobre el espacio de entrada de nuestros datos.
Los resultados obtenidos por nuestro sistema son entonces comparados con aquellos del \emph{Oráculo Optimista} para cada una de las métricas de diversidad utilizadas.

Finalmente, se analiza también en esta etapa el comportamiento del sistema bajo diferentes combinaciones de otros hiper-parámetros como la cantidad de modelos base.
La mejor configuración de dichos hiper-parámetros, incluyendo la métrica de diversidad que mejores resultados proporciona al sistema, son utilizados en la segunda etapa experimental.

La segunda etapa experimental consiste del estudio de la capacidad del sistema para lograr producir modelos de ensemble que son precisos de acuerdo a una función de perdida determinada y justos según una o varias métricas de equidad. 
Con este fin se realiza una comparación entre los resultados obtenidos por nuestro sistema y los obtenidos por varios otros sistemas que resultan relevantes en la literatura para la solución de este problema.

\subsection{Escenarios de Evaluación}

La primera etapa experimental evalúa el sistema en la tarea \emph{HAHA 2019}(\textit{Humor Analysis based on Human Annotation}), con marco en el evento \textit{IberLEF 2019} \parencite{chiruzzo2019overview}.
El proceso de evaluación consiste primero de la ejecución de la primera fase de nuestro sistema, esto es, la exploración del espacio de modelos y obtención del conjunto de modelos base a ser ensamblados.
Posteriormente la segunda fase de nuestro sistema es ejecutada, utilizando como única función objetivo la función de perdida que fue utilizada en la obtención de los modelos base.
La función objetivo a optimizar en ambas fases del sistema es la precisión del sistema.

Finalmente la segunda etapa experimental consiste de la ejecución de extremo a extremo de nuestro sistema.
En esta etapa se utiliza la mejor configuración de hiper-parámetros acorde a los resultados de la primera fase.
Ambas fases optimizan la precisión del modelo, en particular la segunda fase incorpora al proceso de optimización métricas de equidad, dígase \emph{Statistical Parity} y \emph{Equalized Opportunity}, como funciones objetivo adicionales a optimizar simultáneamente con la precisión.

\subsection{Corpus de Evaluación}

Dos conjuntos de datos son utilizados para la evaluación del sistema en los experimentos realizados.

\begin{description}

\item[HAHA 2019]
Colección de datos utilizada en la tarea \emph{HAHA 2019} (\emph{Humor Analysis based on Human Annotation}), con marco en \emph{IberLEF 2019} \parencite{chiruzzo2019overview}.
El corpus contiene $30\,000$ tweets en Español clasificados manualmente, de los cuales $24\,000$ son para entrenamiento y $6\,000$ para evaluación.
Cada uno de estos tweets es clasificado en \emph{humoroso} o \emph{no-humoroso}.

La colección de datos es anotada a partir de asignar una clasificación de \emph{gracioso} o \emph{no-gracioso} a cada tweet, en caso de ser \emph{gracioso} se da una puntuación de $[1,5]$ de cuan \emph{gracioso} es dicho tweet.

\begin{table}[h]
    \centering
    \begin{tabular}{lccc}
    \toprule
                            & Entrenamiento & Evaluación & Total   \\\midrule
        Tweets              & 24\,000       & 6\,000     & 30\,000 \\
        Graciosos           & 9\,253        & 2\,342     & 11\,595 \\
        No graciosos        & 14\,757       & 3\,658     & 18\,405 \\
        Puntuación promedio & 2.04          & 2.03       & 2.04    \\\midrule
        Total de Votos      & 59\,440       & 13\,605    & 73\,045 \\
        Votos 1             & 19\,063       & 4\,818     & 23\,881 \\
        Votos 2             & 14\,713       & 3\,777     & 18\,490 \\
        Votos 3             & 10\,206       & 2\,649     & 12\,855 \\
        Votos 4             &  4\,493       & 1\,122     &  5\,615 \\
        Votos 5             &  1\,305       &    275     &  1\,580 \\
    \bottomrule
    \end{tabular}
    \caption{Composición de los datos según la cantidad de votos para cada clase.}
    \label{table:haha2019info}
\end{table}

\item[Adult]
La colección de datos \emph{Adult} \parencite{ucidata} presenta información extraída del censo de 1994 en los Estados Unidos por Barry Becker.
Los datos contienen detalles personales de los individues, tales como nivel de educación, horas de trabajo a la semana, raza, sexo, etc., y el objetivo es predecir si el individuo ganará un salario mayor a \$$50$K al año.
Hay un total de $48\,842$ filas de datos, y de estas, $3\,620$ contienen casillas con valores desconocidos, dejando $45\,222$ filas completas.
Existen dos clases en las cuales clasificar a los individuos dependiendo de su salario anual, estas son, '$<50K$' o '$\leq50K$'.
Las clases están desbalanceadas, con una tendencia hacia la etiqueta '$\leq50K$', la cual representa aproximadamente el $75\%$ de los ejemplos.

\end{description}

\subsection{Configuración Experimental}

En todos los experimentos, el sistema fue configurado para permitir que cada fase ejecutara a lo sumo $10\,000$ iteraciones o por una hora.
Los parámetros de búsqueda de AutoGOAL fueron los siguientes:
\begin{itemize}
    \item \texttt{popsize=50}
    \item \texttt{selection=10}
    \item \texttt{cross\_validation\_steps=3}
    \item \texttt{validation\_split=0.3}
\end{itemize}

Debido a limitaciones de infraestructura, los algoritmos de \emph{Aprendizaje Profundo} fueron excluidos del conjunto de algoritmos disponibles para AutoGOAL, esto significa que flujos basados en \emph{Keras} y \emph{BERTA fueron excluidos y fundamentalmente} flujos basados en \emph{ScikitLearn} fueron utilizados.
AutoGOAL es una herramienta aun en desarrollo y la ultima versión estable en el momento de realizar los experimentos (\texttt{v0.6.0}) no soporta completamente algoritmos de  \emph{Aprendizaje Profundo} en todas las arquitecturas.

No incluir los algoritmos de \emph{Aprendizaje Profundo} en la configuración experimental puede tener un impacto negativo en el mejor rendimiento que puede ser alcanzado por el sistema.
Es decir, el rendimiento del sistema mientras resuelve el problema puede no ser directamente comparado con otras soluciones reportadas que utilizan técnicas de \emph{Aprendizaje Profundo}.
Sin embargo, la ausencia de estos algoritmos no deberían afectar la capacidad del sistema de mejorar la capacidad de los modelos base que son encontrados en la primera fase.

% TODO: Esto aun es relevante?
%Not including deep learning algorithms in the experimental setup
%may have a negative impact on the maximum performance achieved by the system.
%That is, the scored performance of the system while solving a problem cannot be directly compared to other reported solutions that use deep learning techniques.
%However, the lack of such algorithms should not affect the capacity of the system to improve
%the performance of the pipelines found in the first phase, i.e. the base models.
%Therefore, if  the system improves the performance of the base models, then the top performance is expected to further increase once the deep learning architectures become compatible.
%This is expected to happen not only because the base models will now perform better, but also because more powerful ensemble architectures will be seamlessly added at the same time.
%AutoGOAL has already be proven to achieve competitive results when set up properly~\cite{estevez2020automatic}

\subsubsection{Biblioteca}

El sistema propuesto en este trabajo es parte de la biblioteca en desarrollo \texttt{BFair}\footnote{https://github.com/bfair-ml/bfair}
La biblioteca tiene como objetivo atacar los problemas de sesgos que emergen de entrenar modelos de \emph{Aprendizaje Automático} que en datos que muestran sesgos de los humanos.

\subsubsection{Hardware}

Los experimentos fueron ejecutados en un equipo con las siguientes propiedades:
CPU Intel Core i9-9900K (-MT-MCP-) con velocidad máxima de 3651/5000 MHz, cache de 16384 KB y RAM de 64GB.

\section{Resultados}

A continuación se muestran los resultados de la primera y segunda fase experimental respectivamente, según descritas anteriormente.

\subsection*{Primera Etapa Experimental}